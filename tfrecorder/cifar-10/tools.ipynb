{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(layer_name, x, out_channels, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=True):\n",
    "    '''\n",
    "    Convolution op wrapper, use RELU activation after convolution\n",
    "    \n",
    "    layer_name: \n",
    "    x: input tensor, [batch_size, height, width, channels]\n",
    "    out_channels: number of filter ()\n",
    "    kernel_size: conv kernel size [3,3]\n",
    "    stride: \n",
    "    is_pertrain: 是否需要forze，True is train  false is forze\n",
    "    \n",
    "    '''\n",
    "    in_channels = x.get_shape()[-1]\n",
    "    \n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable(name='weights',\n",
    "                            trainable=is_pretrain,\n",
    "                            shape=[kernel_size[0], kernel_size[1], in_channels, out_channels],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer()) # default is uniform distribution initialization\n",
    "        b = tf.get_variable(name='biases',\n",
    "                            trainable=is_pretrain,\n",
    "                            shape=[out_channels],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        x = tf.nn.conv2d(x, w, stride, padding='SAME', name='conv')\n",
    "        x = tf.nn.bias_add(x, b, name='bias_add')\n",
    "        x = tf.nn.relu(x, name='relu')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool(layer_name, x, kernel=[1,2,2,1], stride=[1,2,2,1], is_max_pool=True):\n",
    "    '''\n",
    "    pooling \n",
    "    \n",
    "    x: input tensor [batch_size, height, width, channels]\n",
    "    kernel: pooling kernel [1,2,2,1], the size of kernel is 2X2\n",
    "    stride: stride size, VGG paper used [1,2,2,1]\n",
    "    padding:\n",
    "    is_max_pool: boolen\n",
    "            if True: use max pooling\n",
    "            else: use avg pooling\n",
    "    '''\n",
    "    if is_max_pool:\n",
    "        x = tf.nn.max_pool(x, kernel, strides=stride, padding='SAME', name=layer_name)\n",
    "    else:\n",
    "        x = tf.nn.avg_pool(x, kernel, strides=stride, padding='SAME', name=layer_name)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(x):\n",
    "    '''\n",
    "    Batch normlization\n",
    "    '''\n",
    "    epsilon = 1e-3\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0])\n",
    "    x = tf.nn.batch_normalization(x,\n",
    "                                  mean=batch_mean,\n",
    "                                  variance=batch_var,\n",
    "                                  offset=None,\n",
    "                                  scale=None,\n",
    "                                  variance_epsilon=epsilon)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FC_layer(layer_name, x, out_nodes):\n",
    "    '''Wrapper for fully connected layers with RELU activation as default\n",
    "    Args:\n",
    "        layer_name: e.g. 'FC1', 'FC2'\n",
    "        x: input feature map\n",
    "        out_nodes: number of neurons for current FC layer\n",
    "    '''\n",
    "    shape = x.get_shape()\n",
    "    if len(shape) == 4:\n",
    "        size = shape[1].value * shape[2].value * shape[3].value\n",
    "    else:\n",
    "        size = shape[-1].value\n",
    "\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable('weights',\n",
    "                            shape=[size, out_nodes],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('biases',\n",
    "                            shape=[out_nodes],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        flat_x = tf.reshape(x, [-1, size]) # flatten into 1D\n",
    "        \n",
    "        x = tf.nn.bias_add(tf.matmul(flat_x, w), b)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    '''Compute loss\n",
    "    Args:\n",
    "        logits: logits tensor, [batch_size, n_classes]\n",
    "        labels: should be one-hot labels\n",
    "    '''\n",
    "    with tf.name_scope('loss') as scope:\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels,name='cross-entropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        tf.summary.scalar(scope+'/loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(loss, learning_rate, global_step):\n",
    "    '''optimization, use Gradient Descent as default\n",
    "    '''\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "    '''\n",
    "    Evaluate the quality of the logits at predicting the label.\n",
    "    \n",
    "  Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, \n",
    "    '''\n",
    "    with tf.name_scope('accuracy') as scope:\n",
    "        correct = tf.equal(tf.arg_max(logits, 1), tf.arg_max(labels, 1))\n",
    "        correct = tf.cast(correct, tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct)*100.0\n",
    "        tf.summary.scalar(scope+'/accuracy', accuracy)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_correct_prediction(logits, labels):\n",
    "    '''\n",
    "    Evaluate the quality of the logits at predicting the label.\n",
    "    Return:\n",
    "      the number of correct predictions\n",
    "    '''\n",
    "    correct = tf.equal(tf.arg_max(logits, 1), tf.arg_max(labels, 1))\n",
    "    correct = tf.cast(correct, tf.int32)\n",
    "    n_correct = tf.reduce_sum(correct)\n",
    "    \n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(data_path, session):\n",
    "    data_dict = np.load(data_path, encoding='latin1').item()\n",
    "    \n",
    "    keys = sorted(data_dict.keys())\n",
    "    for key in keys:\n",
    "        with tf.variable_scope(key, reuse=True):\n",
    "            for subkey, data in zip(('weights', 'biases'), data_dict[key]):\n",
    "                session.run(tf.get_variable(subkey).assign(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_load(data_path):\n",
    "    \n",
    "    data_dict = np.load(data_path, encoding='latin1').item()\n",
    "    keys = sorted(data_dict.keys())\n",
    "    for key in keys:\n",
    "        weights = data_dict[key][0]\n",
    "        biases = data_dict[key][1]\n",
    "        print('\\n')\n",
    "        print(key)\n",
    "        print('weights shape: ', weights.shape)\n",
    "        print('biases shape: ', biases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../vgg16.npy'\n",
    "test_load(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_with_skip(data_path, session, skip_layer):\n",
    "    data_dict = np.load(data_path, encoding='latin1').item()\n",
    "    for key in data_dict:\n",
    "        if key not in skip_layer:\n",
    "            with tf.variable_scope(key, reuse=True):\n",
    "                for subkey, data in zip(('weights', 'biases'), data_dict[key]):\n",
    "                    session.run(tf.get_variable(subkey).assign(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
